{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c66a7a0b",
   "metadata": {},
   "source": [
    "## Stable Diffusion 모델 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467fd7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3.5-large-turbo\", torch_dtype=torch.bfloat16)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "image = pipe(\n",
    "    \"A capybara holding a sign that reads Hello Fast World\",\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=0.0,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff47ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "image = pipe(\n",
    "    prompt=\"a photo of a cat holding a sign that says hello world\",\n",
    "    negative_prompt=\"\",\n",
    "    num_inference_steps=28,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=7.0,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238451c",
   "metadata": {},
   "source": [
    "### 코드 더 자세히 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ead4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "\n",
    "from transformers import (\n",
    "    CLIPTextModelWithProjection,\n",
    "    CLIPTokenizer,\n",
    "    SiglipImageProcessor,\n",
    "    SiglipVisionModel,\n",
    "    T5EncoderModel,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "from diffusers.models.autoencoders import AutoencoderKL\n",
    "from diffusers.models.transformers import SD3Transformer2DModel\n",
    "from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n",
    "from diffusers.pipelines.stable_diffusion_3.pipeline_outputs import StableDiffusion3PipelineOutput\n",
    "\n",
    "\n",
    "# Copied from diffusers.pipelines.flux.pipeline_flux.calculate_shift\n",
    "def calculate_shift(\n",
    "    image_seq_len,\n",
    "    base_seq_len: int = 256,\n",
    "    max_seq_len: int = 4096,\n",
    "    base_shift: float = 0.5,\n",
    "    max_shift: float = 1.15,\n",
    "):\n",
    "    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n",
    "    b = base_shift - m * base_seq_len\n",
    "    mu = image_seq_len * m + b\n",
    "    return mu\n",
    "\n",
    "\n",
    "# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n",
    "def retrieve_timesteps(\n",
    "    scheduler,\n",
    "    num_inference_steps: Optional[int] = None,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    timesteps: Optional[List[int]] = None,\n",
    "    sigmas: Optional[List[float]] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    r\"\"\"\n",
    "    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n",
    "    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n",
    "\n",
    "    Args:\n",
    "        scheduler (`SchedulerMixin`):\n",
    "            The scheduler to get timesteps from.\n",
    "        num_inference_steps (`int`):\n",
    "            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n",
    "            must be `None`.\n",
    "        device (`str` or `torch.device`, *optional*):\n",
    "            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n",
    "        timesteps (`List[int]`, *optional*):\n",
    "            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n",
    "            `num_inference_steps` and `sigmas` must be `None`.\n",
    "        sigmas (`List[float]`, *optional*):\n",
    "            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n",
    "            `num_inference_steps` and `timesteps` must be `None`.\n",
    "\n",
    "    Returns:\n",
    "        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n",
    "        second element is the number of inference steps.\n",
    "    \"\"\"\n",
    "    if timesteps is not None and sigmas is not None:\n",
    "        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n",
    "    if timesteps is not None:\n",
    "        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accepts_timesteps:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    elif sigmas is not None:\n",
    "        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accept_sigmas:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    else:\n",
    "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "    return timesteps, num_inference_steps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SimpleStableDiffusion3Pipeline():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer: SD3Transformer2DModel,\n",
    "        scheduler: FlowMatchEulerDiscreteScheduler,\n",
    "        vae: AutoencoderKL,\n",
    "        text_encoder: CLIPTextModelWithProjection,\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        text_encoder_2: CLIPTextModelWithProjection,\n",
    "        tokenizer_2: CLIPTokenizer,\n",
    "        text_encoder_3: T5EncoderModel,\n",
    "        tokenizer_3: T5TokenizerFast,\n",
    "        image_encoder: SiglipVisionModel = None,\n",
    "        feature_extractor: SiglipImageProcessor = None,\n",
    "    ):\n",
    "        super(SimpleStableDiffusion3Pipeline, self).__init__(\n",
    "            transformer,\n",
    "            scheduler,\n",
    "            vae,\n",
    "            text_encoder,\n",
    "            tokenizer,\n",
    "            text_encoder_2,\n",
    "            tokenizer_2,\n",
    "            text_encoder_3,\n",
    "            tokenizer_3,\n",
    "            image_encoder,\n",
    "            feature_extractor\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        prompt_3: Optional[Union[str, List[str]]] = None,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        num_inference_steps: int = 28,\n",
    "        sigmas: Optional[List[float]] = None,\n",
    "        guidance_scale: float = 7.0,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        negative_prompt_3: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.FloatTensor] = None,\n",
    "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        ip_adapter_image: Optional[PipelineImageInput] = None,\n",
    "        ip_adapter_image_embeds: Optional[torch.Tensor] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        clip_skip: Optional[int] = None,\n",
    "        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n",
    "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
    "        max_sequence_length: int = 256,\n",
    "        skip_guidance_layers: List[int] = None,\n",
    "        skip_layer_guidance_scale: float = 2.8,\n",
    "        skip_layer_guidance_stop: float = 0.2,\n",
    "        skip_layer_guidance_start: float = 0.01,\n",
    "        mu: Optional[float] = None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Function invoked when calling the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n",
    "                instead.\n",
    "            prompt_2 (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts to be sent to `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is\n",
    "                will be used instead\n",
    "            prompt_3 (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts to be sent to `tokenizer_3` and `text_encoder_3`. If not defined, `prompt` is\n",
    "                will be used instead\n",
    "            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
    "                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n",
    "            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
    "                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n",
    "            num_inference_steps (`int`, *optional*, defaults to 50):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            sigmas (`List[float]`, *optional*):\n",
    "                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n",
    "                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n",
    "                will be used.\n",
    "            guidance_scale (`float`, *optional*, defaults to 7.0):\n",
    "                Guidance scale as defined in [Classifier-Free Diffusion\n",
    "                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n",
    "                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n",
    "                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n",
    "                the text `prompt`, usually at the expense of lower image quality.\n",
    "            negative_prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n",
    "                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n",
    "                less than `1`).\n",
    "            negative_prompt_2 (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and\n",
    "                `text_encoder_2`. If not defined, `negative_prompt` is used instead\n",
    "            negative_prompt_3 (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation to be sent to `tokenizer_3` and\n",
    "                `text_encoder_3`. If not defined, `negative_prompt` is used instead\n",
    "            num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate per prompt.\n",
    "            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
    "                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n",
    "                to make generation deterministic.\n",
    "            latents (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n",
    "                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
    "                tensor will ge generated by sampling using the supplied random `generator`.\n",
    "            prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
    "                provided, text embeddings will be generated from `prompt` input argument.\n",
    "            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
    "                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n",
    "                argument.\n",
    "            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.\n",
    "                If not provided, pooled text embeddings will be generated from `prompt` input argument.\n",
    "            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
    "                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`\n",
    "                input argument.\n",
    "            ip_adapter_image (`PipelineImageInput`, *optional*):\n",
    "                Optional image input to work with IP Adapters.\n",
    "            ip_adapter_image_embeds (`torch.Tensor`, *optional*):\n",
    "                Pre-generated image embeddings for IP-Adapter. Should be a tensor of shape `(batch_size, num_images,\n",
    "                emb_dim)`. It should contain the negative image embedding if `do_classifier_free_guidance` is set to\n",
    "                `True`. If not provided, embeddings are computed from the `ip_adapter_image` input argument.\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generate image. Choose between\n",
    "                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.stable_diffusion_3.StableDiffusion3PipelineOutput`] instead of\n",
    "                a plain tuple.\n",
    "            joint_attention_kwargs (`dict`, *optional*):\n",
    "                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n",
    "                `self.processor` in\n",
    "                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n",
    "            callback_on_step_end (`Callable`, *optional*):\n",
    "                A function that calls at the end of each denoising steps during the inference. The function is called\n",
    "                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n",
    "                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n",
    "                `callback_on_step_end_tensor_inputs`.\n",
    "            callback_on_step_end_tensor_inputs (`List`, *optional*):\n",
    "                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n",
    "                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n",
    "                `._callback_tensor_inputs` attribute of your pipeline class.\n",
    "            max_sequence_length (`int` defaults to 256): Maximum sequence length to use with the `prompt`.\n",
    "            skip_guidance_layers (`List[int]`, *optional*):\n",
    "                A list of integers that specify layers to skip during guidance. If not provided, all layers will be\n",
    "                used for guidance. If provided, the guidance will only be applied to the layers specified in the list.\n",
    "                Recommended value by StabiltyAI for Stable Diffusion 3.5 Medium is [7, 8, 9].\n",
    "            skip_layer_guidance_scale (`int`, *optional*): The scale of the guidance for the layers specified in\n",
    "                `skip_guidance_layers`. The guidance will be applied to the layers specified in `skip_guidance_layers`\n",
    "                with a scale of `skip_layer_guidance_scale`. The guidance will be applied to the rest of the layers\n",
    "                with a scale of `1`.\n",
    "            skip_layer_guidance_stop (`int`, *optional*): The step at which the guidance for the layers specified in\n",
    "                `skip_guidance_layers` will stop. The guidance will be applied to the layers specified in\n",
    "                `skip_guidance_layers` until the fraction specified in `skip_layer_guidance_stop`. Recommended value by\n",
    "                StabiltyAI for Stable Diffusion 3.5 Medium is 0.2.\n",
    "            skip_layer_guidance_start (`int`, *optional*): The step at which the guidance for the layers specified in\n",
    "                `skip_guidance_layers` will start. The guidance will be applied to the layers specified in\n",
    "                `skip_guidance_layers` from the fraction specified in `skip_layer_guidance_start`. Recommended value by\n",
    "                StabiltyAI for Stable Diffusion 3.5 Medium is 0.01.\n",
    "            mu (`float`, *optional*): `mu` value used for `dynamic_shifting`.\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.stable_diffusion_3.StableDiffusion3PipelineOutput`] or `tuple`:\n",
    "            [`~pipelines.stable_diffusion_3.StableDiffusion3PipelineOutput`] if `return_dict` is True, otherwise a\n",
    "            `tuple`. When returning a tuple, the first element is a list with the generated images.\n",
    "        \"\"\"\n",
    "\n",
    "        height = height or self.default_sample_size * self.vae_scale_factor\n",
    "        width = width or self.default_sample_size * self.vae_scale_factor\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(\n",
    "            prompt,\n",
    "            prompt_2,\n",
    "            prompt_3,\n",
    "            height,\n",
    "            width,\n",
    "            negative_prompt=negative_prompt,\n",
    "            negative_prompt_2=negative_prompt_2,\n",
    "            negative_prompt_3=negative_prompt_3,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "        )\n",
    "\n",
    "        self._guidance_scale = guidance_scale\n",
    "        self._skip_layer_guidance_scale = skip_layer_guidance_scale\n",
    "        self._clip_skip = clip_skip\n",
    "        self._joint_attention_kwargs = joint_attention_kwargs\n",
    "        self._interrupt = False\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        lora_scale = (\n",
    "            self.joint_attention_kwargs.get(\"scale\", None) if self.joint_attention_kwargs is not None else None\n",
    "        )\n",
    "        (\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "        ) = self.encode_prompt(\n",
    "            prompt=prompt,\n",
    "            prompt_2=prompt_2,\n",
    "            prompt_3=prompt_3,\n",
    "            negative_prompt=negative_prompt,\n",
    "            negative_prompt_2=negative_prompt_2,\n",
    "            negative_prompt_3=negative_prompt_3,\n",
    "            do_classifier_free_guidance=self.do_classifier_free_guidance,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "            device=device,\n",
    "            clip_skip=self.clip_skip,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            lora_scale=lora_scale,\n",
    "        )\n",
    "\n",
    "        if self.do_classifier_free_guidance:\n",
    "            if skip_guidance_layers is not None:\n",
    "                original_prompt_embeds = prompt_embeds\n",
    "                original_pooled_prompt_embeds = pooled_prompt_embeds\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
    "            pooled_prompt_embeds = torch.cat([negative_pooled_prompt_embeds, pooled_prompt_embeds], dim=0)\n",
    "\n",
    "        # 4. Prepare latent variables\n",
    "        num_channels_latents = self.transformer.config.in_channels\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "\n",
    "        # 5. Prepare timesteps\n",
    "        scheduler_kwargs = {}\n",
    "        if self.scheduler.config.get(\"use_dynamic_shifting\", None) and mu is None:\n",
    "            _, _, height, width = latents.shape\n",
    "            image_seq_len = (height // self.transformer.config.patch_size) * (\n",
    "                width // self.transformer.config.patch_size\n",
    "            )\n",
    "            mu = calculate_shift(\n",
    "                image_seq_len,\n",
    "                self.scheduler.config.get(\"base_image_seq_len\", 256),\n",
    "                self.scheduler.config.get(\"max_image_seq_len\", 4096),\n",
    "                self.scheduler.config.get(\"base_shift\", 0.5),\n",
    "                self.scheduler.config.get(\"max_shift\", 1.16),\n",
    "            )\n",
    "            scheduler_kwargs[\"mu\"] = mu\n",
    "        elif mu is not None:\n",
    "            scheduler_kwargs[\"mu\"] = mu\n",
    "        timesteps, num_inference_steps = retrieve_timesteps(\n",
    "            self.scheduler,\n",
    "            num_inference_steps,\n",
    "            device,\n",
    "            sigmas=sigmas,\n",
    "            **scheduler_kwargs,\n",
    "        )\n",
    "        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
    "        self._num_timesteps = len(timesteps)\n",
    "\n",
    "        # 6. Prepare image embeddings\n",
    "        if (ip_adapter_image is not None and self.is_ip_adapter_active) or ip_adapter_image_embeds is not None:\n",
    "            ip_adapter_image_embeds = self.prepare_ip_adapter_image_embeds(\n",
    "                ip_adapter_image,\n",
    "                ip_adapter_image_embeds,\n",
    "                device,\n",
    "                batch_size * num_images_per_prompt,\n",
    "                self.do_classifier_free_guidance,\n",
    "            )\n",
    "\n",
    "            if self.joint_attention_kwargs is None:\n",
    "                self._joint_attention_kwargs = {\"ip_adapter_image_embeds\": ip_adapter_image_embeds}\n",
    "            else:\n",
    "                self._joint_attention_kwargs.update(ip_adapter_image_embeds=ip_adapter_image_embeds)\n",
    "\n",
    "        # 7. Denoising loop\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                if self.interrupt:\n",
    "                    continue\n",
    "\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
    "                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "                timestep = t.expand(latent_model_input.shape[0])\n",
    "\n",
    "                noise_pred = self.transformer(\n",
    "                    hidden_states=latent_model_input,\n",
    "                    timestep=timestep,\n",
    "                    encoder_hidden_states=prompt_embeds,\n",
    "                    pooled_projections=pooled_prompt_embeds,\n",
    "                    joint_attention_kwargs=self.joint_attention_kwargs,\n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "\n",
    "                # perform guidance\n",
    "                if self.do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "                    should_skip_layers = (\n",
    "                        True\n",
    "                        if i > num_inference_steps * skip_layer_guidance_start\n",
    "                        and i < num_inference_steps * skip_layer_guidance_stop\n",
    "                        else False\n",
    "                    )\n",
    "                    if skip_guidance_layers is not None and should_skip_layers:\n",
    "                        timestep = t.expand(latents.shape[0])\n",
    "                        latent_model_input = latents\n",
    "                        noise_pred_skip_layers = self.transformer(\n",
    "                            hidden_states=latent_model_input,\n",
    "                            timestep=timestep,\n",
    "                            encoder_hidden_states=original_prompt_embeds,\n",
    "                            pooled_projections=original_pooled_prompt_embeds,\n",
    "                            joint_attention_kwargs=self.joint_attention_kwargs,\n",
    "                            return_dict=False,\n",
    "                            skip_layers=skip_guidance_layers,\n",
    "                        )[0]\n",
    "                        noise_pred = (\n",
    "                            noise_pred + (noise_pred_text - noise_pred_skip_layers) * self._skip_layer_guidance_scale\n",
    "                        )\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents_dtype = latents.dtype\n",
    "                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "\n",
    "                if latents.dtype != latents_dtype:\n",
    "                    if torch.backends.mps.is_available():\n",
    "                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n",
    "                        latents = latents.to(latents_dtype)\n",
    "\n",
    "                if callback_on_step_end is not None:\n",
    "                    callback_kwargs = {}\n",
    "                    for k in callback_on_step_end_tensor_inputs:\n",
    "                        callback_kwargs[k] = locals()[k]\n",
    "                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
    "\n",
    "                    latents = callback_outputs.pop(\"latents\", latents)\n",
    "                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
    "                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n",
    "                    negative_pooled_prompt_embeds = callback_outputs.pop(\n",
    "                        \"negative_pooled_prompt_embeds\", negative_pooled_prompt_embeds\n",
    "                    )\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "\n",
    "        if output_type == \"latent\":\n",
    "            image = latents\n",
    "\n",
    "        else:\n",
    "            latents = (latents / self.vae.config.scaling_factor) + self.vae.config.shift_factor\n",
    "\n",
    "            image = self.vae.decode(latents, return_dict=False)[0]\n",
    "            image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "        # Offload all models\n",
    "        self.maybe_free_model_hooks()\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return StableDiffusion3PipelineOutput(images=image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c1eff2",
   "metadata": {},
   "source": [
    "### 부록: SDXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc003ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "# load both base & refiner\n",
    "base = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "base.to(\"cuda\")\n",
    "refiner = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=base.text_encoder_2,\n",
    "    vae=base.vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "refiner.to(\"cuda\")\n",
    "\n",
    "# Define how many steps and what % of steps to be run on each experts (80/20) here\n",
    "n_steps = 40\n",
    "high_noise_frac = 0.8\n",
    "\n",
    "prompt = \"A majestic lion jumping from a big stone at night\"\n",
    "\n",
    "# run both experts\n",
    "image = base(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=n_steps,\n",
    "    denoising_end=high_noise_frac,\n",
    "    output_type=\"latent\",\n",
    ").images\n",
    "image = refiner(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=n_steps,\n",
    "    denoising_start=high_noise_frac,\n",
    "    image=image,\n",
    ").images[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
