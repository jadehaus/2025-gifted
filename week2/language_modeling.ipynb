{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 언어 모델의 기초적 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 다운로드 받고 살펴보기\n",
    "\n",
    "`wikipedia-api`는 인터넷에서 위키피디아 문서를 불러올 수 있도록 도와줍니다. `google colab` 환경에서 실행할 경우, 아래 코드를 실행해 라이브러리를 다운받아봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ihSgFiA4pQt9",
    "outputId": "dd2d5cf2-eae7-4469-8916-0831a845ae62"
   },
   "outputs": [],
   "source": [
    "!pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iOpuatz3ollk",
    "outputId": "943d9fc5-eb0d-44fd-a5c0-e0a03755d82b"
   },
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia('MyProjectName', 'en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "p_wiki = wiki_wiki.page(\"Breakfast\")\n",
    "text = p_wiki.text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "\n",
    "인터넷에서 텍스트를 그대로 다운받아 사용하면 대부분의 경우 텍스트 데이터가 잘 정돈되어 있지 않습니다.\n",
    "따라서 불필요한 텍스트를 제거하고, 학습에 용이한 형태로 텍스트를 수정해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvPI7Qjro3en"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_text_to_sentences(text):\n",
    "    # A basic sentence tokenizer\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return sentences\n",
    "\n",
    "def remove_text_from_start_end_marker(text, start_marker='(', end_marker=')'):\n",
    "    # Remove parentheses and their content\n",
    "    return re.sub(r'\\{}.*?\\{}'.format(re.escape(start_marker), re.escape(end_marker)), '', text).strip()\n",
    "\n",
    "def clean_text_data(text):\n",
    "\n",
    "    sentences = split_text_to_sentences(text)\n",
    "    sentences = [i.lower() for i in sentences] # make sentence lower cased. e.g. \"Hello World\" -> \"hello world\"\n",
    "    sentences = [remove_text_from_start_end_marker(i) for i in sentences] # remove parentheses and their content. e.g. \"hello world (test)\" -> \"hello world\"\n",
    "\n",
    "    # Some sentences are just too long.\n",
    "    # We will split them into smaller sentences.\n",
    "    short_sentences = []\n",
    "    for i in sentences:\n",
    "        temp = i.split(',')\n",
    "        for j in temp:\n",
    "            short_sentences.append(j.strip())\n",
    "\n",
    "    # Remove undesirable characters\n",
    "    to_replace = [\"!\", \";\", '\\n', '</p>', '<a', 'id=', \"href=\", 'title=', 'class=', '</a>', '(', ')', '}', '{',\n",
    "                  '</sup>', '<p>', '</b>', '<sup', '>', '<', '\\\\', '-']\n",
    "    replace_with = ''\n",
    "\n",
    "    cleaned_sentences = []\n",
    "    for i in short_sentences:\n",
    "        word_array = i.split()\n",
    "        word_array_new = []\n",
    "        for word in word_array:\n",
    "            for to_replace_val in to_replace:\n",
    "                word = word.replace(to_replace_val, replace_with)\n",
    "            word_array_new.append(word)\n",
    "        cleaned_sentence = ' '.join(word_array_new).strip()\n",
    "        cleaned_sentence = re.sub(r'\\s+', ' ', cleaned_sentence) # Remove extra whitespaces\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "\n",
    "    # Now some sentences are too short.\n",
    "    # We will remove them.\n",
    "    cleaned_sentences = [i for i in cleaned_sentences if len(i.split()) > 10]\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = clean_text_data(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음 단어가 등장할 확률 계산\n",
    "\n",
    "이제 우리는 언어 모델의 기초를 살펴봅니다. \n",
    "언어 모델은 지금까지 생성된 텍스트를 바탕으로 다음에 등장할 단어(토큰)의 확률을 토대로 새로운 단어를 생성합니다.  \n",
    "\n",
    "예를 들어, 가장 단순한 형태의 언어모델은, 이전 단어 다음에 등장할 가장 높은 확률의 단어를 텍스트에서 구해서 그 단어를 다음 단어로 채택합니다. \n",
    "\n",
    "먼저, 각 단어의 다음으로 등장하는 단어를 전부 세어서, 다음에 등장할 단어의 확률을 계산해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8oFCVOrrhUZ",
    "outputId": "e806933c-9c45-4044-b7dd-242b8d34a55a"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Hugging Face tokenizer (you can change the model name!)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def compute_next_token_probabilities(sentences, given_token_text, tokenizer=None):\n",
    "    # Check if tokenizer is provided\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # Tokenize all sentences into token ids\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        token_ids = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        tokens.extend(token_ids)\n",
    "\n",
    "    # Convert the given token text to token id\n",
    "    given_token_id = tokenizer.convert_tokens_to_ids(given_token_text)\n",
    "\n",
    "    # Dictionary to store next-token counts\n",
    "    next_token_counts = defaultdict(Counter)\n",
    "\n",
    "    # Populate next-token counts\n",
    "    for current_token, next_token in zip(tokens[:-1], tokens[1:]):\n",
    "        next_token_counts[current_token][next_token] += 1\n",
    "\n",
    "    # Calculate probabilities\n",
    "    total_next = sum(next_token_counts[given_token_id].values())\n",
    "    if total_next == 0:\n",
    "        return {}\n",
    "\n",
    "    probabilities = {\n",
    "        tokenizer.convert_ids_to_tokens(token_id): count / total_next\n",
    "        for token_id, count in next_token_counts[given_token_id].items()\n",
    "    }\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_token_text = 'breakfast'\n",
    "probabilities = compute_next_token_probabilities(sentences, given_token_text)\n",
    "\n",
    "# Output probabilities\n",
    "for next_token, prob in sorted(probabilities.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"'{given_token_text}' → '{next_token}': {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 생성하기\n",
    "\n",
    "이제 우리는 기초적인 언어 모델을 완성했습니다.  \n",
    "\n",
    "매번 단어를 생성할 때마다, 이전 단어 다음에 가장 높은 확률로 등장할 단어를 생성하도록 선택해서 문장을 이어나가 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0Lr2taqvnqj",
    "outputId": "cddfd0b3-a0b8-42ba-dd4c-6dd3a36d0489"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def compute_next_token_counts(tokens):\n",
    "    next_token_counts = defaultdict(Counter)\n",
    "    for current_token, next_token in zip(tokens[:-1], tokens[1:]):\n",
    "        next_token_counts[current_token][next_token] += 1\n",
    "    return next_token_counts\n",
    "\n",
    "def prepare_token_data(sentences, tokenizer=None):\n",
    "    # Tokenize all sentences into token ids\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        token_ids = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        tokens.extend(token_ids)\n",
    "    return tokens\n",
    "\n",
    "def greedy_generate_sentence(sentences, start_token_text, tokenizer=None, max_length=20):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokens = prepare_token_data(sentences, tokenizer=tokenizer)\n",
    "    next_token_counts = compute_next_token_counts(tokens)\n",
    "\n",
    "    # Start token\n",
    "    current_token_id = tokenizer.convert_tokens_to_ids(start_token_text)\n",
    "    if current_token_id == tokenizer.unk_token_id:\n",
    "        print(f\"Warning: '{start_token_text}' is unknown in the tokenizer vocabulary.\")\n",
    "\n",
    "    generated_tokens = [current_token_id]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Get next token counts\n",
    "        next_counts = next_token_counts.get(current_token_id, None)\n",
    "        if not next_counts:\n",
    "            break  # No next token found\n",
    "\n",
    "        # Greedily pick the most probable next token\n",
    "        next_token_id = next_counts.most_common(1)[0][0]\n",
    "        generated_tokens.append(next_token_id)\n",
    "\n",
    "        # Update current token\n",
    "        current_token_id = next_token_id\n",
    "\n",
    "        # Optional: break if punctuation token or special token is reached\n",
    "        token_text = tokenizer.convert_ids_to_tokens(current_token_id)\n",
    "        if token_text in ['.', '!', '?', tokenizer.sep_token, tokenizer.pad_token]:\n",
    "            break\n",
    "\n",
    "    # Convert token ids back to text\n",
    "    generated_text = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(generated_tokens))\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "start_token_text = 'breakfast'\n",
    "generated_sentence = greedy_generate_sentence(sentences, start_token_text)\n",
    "print(\"Generated sentence:\")\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제점 이해하기\n",
    "\n",
    "문장을 생성하고 보니, 여러가지 문제점이 보입니다:\n",
    "\n",
    "1. 항상 같은 문장만 생성됩니다. 다음으로 등장할 확률이 가장 높은 단어만 탐욕적으로 선택하기 때문에, 문장이 바뀌지 않습니다.\n",
    "2. 무한히 반복되는 문장이 생성될 때도 있습니다. 예를 들어, `너를` 다음에 `사랑하지만`가 가장 빈번하게 등장하고, 이어서 `사랑하지만` 다음에는 다시 `너를`이 등장할 확률이 가장 높다면, 문장은 `너를 사랑하지만 너를 사랑하지만...` 처럼 무한히 반복됩니다.\n",
    "\n",
    "이걸 해결할 수 있는 방법은 무엇일까요? \n",
    "\n",
    "두 가지 문제점을 동시에 해결할 수 있는 가장 간단한 방법은, 다음으로 등장할 가장 높은 확률의 단어만 선택하는 것이 아니라, 조금 확률이 낮은 단어라도 생성될 수 있도록 확률을 토대로 랜덤하게 고르는 것입니다. 한번 해결해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GqzZB-UMvw-3",
    "outputId": "1db12b06-31e5-4361-c6a1-ea341980ef43"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def sample_next_token(next_counts):\n",
    "    tokens, counts = zip(*next_counts.items())\n",
    "    total = sum(counts)\n",
    "    probabilities = [count / total for count in counts]\n",
    "    return random.choices(tokens, weights=probabilities, k=1)[0]\n",
    "\n",
    "def random_sample_generate_sentence(sentences, start_token_text, tokenizer=None, max_length=20):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokens = prepare_token_data(sentences, tokenizer=tokenizer)\n",
    "    next_token_counts = compute_next_token_counts(tokens)\n",
    "\n",
    "    # Start token\n",
    "    current_token_id = tokenizer.convert_tokens_to_ids(start_token_text)\n",
    "    if current_token_id == tokenizer.unk_token_id:\n",
    "        print(f\"Warning: '{start_token_text}' is unknown in the tokenizer vocabulary.\")\n",
    "\n",
    "    generated_tokens = [current_token_id]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Get next token counts\n",
    "        next_counts = next_token_counts.get(current_token_id, None)\n",
    "        if not next_counts:\n",
    "            break  # No next token found\n",
    "\n",
    "        # Sample next token from distribution\n",
    "        next_token_id = sample_next_token(next_counts)\n",
    "        generated_tokens.append(next_token_id)\n",
    "\n",
    "        # Update current token\n",
    "        current_token_id = next_token_id\n",
    "\n",
    "        # Optional: stop if punctuation token or special token\n",
    "        token_text = tokenizer.convert_ids_to_tokens(current_token_id)\n",
    "        if token_text in ['.', '!', '?', tokenizer.sep_token, tokenizer.pad_token]:\n",
    "            break\n",
    "\n",
    "    # Convert token ids back to text\n",
    "    generated_text = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(generated_tokens))\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "start_token_text = 'breakfast'\n",
    "generated_sentence = random_sample_generate_sentence(sentences, start_token_text)\n",
    "print(\"Generated sentence (random sample):\")\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 더 발전할 방향 논의해보기\n",
    "\n",
    "그래도 몇 가지 문제점이 보입니다. 한 번 같이 논의해보고, 앞으로 어떻게 수정할 수 있을지 고민해봅시다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "hdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
