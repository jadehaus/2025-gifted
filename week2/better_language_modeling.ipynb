{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyyvNvcGfxgM"
      },
      "source": [
        "# 언어 모델의 기초적 이해 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu_seUZifxgN"
      },
      "source": [
        "### 지난 시간 복습\n",
        "\n",
        "지난 시간에 우리는 텍스트 데이터를 분석해서, 각 단어 다음에 등장할 확률을 직접 세어서 구한 다음, 계산한 확률을 토대로 문장을 생성해 나갔습니다.\n",
        "\n",
        "여기에서, 우리는 수많은 문제점을 체험할 수 있었습니다:\n",
        "\n",
        "1. 우리가 가지고 있는 텍스트 데이터에 시작 단어가 존재하지 않는 경우, 문장을 생성하지 못합니다.\n",
        "2. 문장을 생성하더라도, 문맥의 흐름을 파악하지 못하고 자연스럽지 않은 문장이 생성되기도 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 언어 모델 토큰화 다시 복습하기"
      ],
      "metadata": {
        "id": "KKNlAFaJf3Da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "sample_text = \"선생님 너무 잘생겼어요!\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")\n",
        "\n",
        "tokens = tokenizer.tokenize(sample_text)\n",
        "print(tokens)\n",
        "\n",
        "actual_tokens = tokenizer(sample_text, return_tensors=\"pt\").input_ids\n",
        "print(actual_tokens)"
      ],
      "metadata": {
        "id": "wGngs8Rkf6a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw2PWERLfxgO"
      },
      "source": [
        "### 지난 시간의 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdJtieC1fxgO"
      },
      "outputs": [],
      "source": [
        "!pip install wikipedia-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TftuXIUKfxgP"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def split_text_to_sentences(text):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "    return sentences\n",
        "\n",
        "def remove_text_from_start_end_marker(text, start_marker='(', end_marker=')'):\n",
        "    return re.sub(r'\\{}.*?\\{}'.format(re.escape(start_marker), re.escape(end_marker)), '', text).strip()\n",
        "\n",
        "def clean_text_data(text):\n",
        "    print(\"Cleaning text data...\")\n",
        "    sentences = split_text_to_sentences(text)\n",
        "    sentences = [i.lower() for i in sentences] # make sentence lower cased. e.g. \"Hello World\" -> \"hello world\"\n",
        "    sentences = [remove_text_from_start_end_marker(i) for i in sentences] # remove parentheses and their content. e.g. \"hello world (test)\" -> \"hello world\"\n",
        "    to_replace = [\"!\", \";\", '\\n', '</p>', '<a', 'id=', \"href=\", 'title=', 'class=', '</a>', '(', ')', '}', '{',\n",
        "                  '</sup>', '<p>', '</b>', '<sup', '>', '<', '\\\\', '-']\n",
        "    replace_with = ''\n",
        "    cleaned_sentences = []\n",
        "    for i in sentences:\n",
        "        word_array = i.split()\n",
        "        word_array_new = []\n",
        "        for word in word_array:\n",
        "            for to_replace_val in to_replace:\n",
        "                word = word.replace(to_replace_val, replace_with)\n",
        "            word_array_new.append(word)\n",
        "        cleaned_sentence = ' '.join(word_array_new).strip()\n",
        "        cleaned_sentence = re.sub(r'\\s+', ' ', cleaned_sentence) # Remove extra whitespaces\n",
        "        cleaned_sentences.append(cleaned_sentence)\n",
        "    print(\"Cleaning complete.\")\n",
        "    return cleaned_sentences\n",
        "\n",
        "def compute_next_token_probabilities(sentences, given_token_text, tokenizer=None):\n",
        "    if tokenizer is None:\n",
        "        print(\"No tokenizer provided. Creating a new tokenizer.\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    tokens = []\n",
        "    for sentence in sentences:\n",
        "        token_ids = tokenizer.encode(sentence, add_special_tokens=False)\n",
        "        tokens.extend(token_ids)\n",
        "    given_token_id = tokenizer.convert_tokens_to_ids(given_token_text)\n",
        "    next_token_counts = defaultdict(Counter)\n",
        "    for current_token, next_token in zip(tokens[:-1], tokens[1:]):\n",
        "        next_token_counts[current_token][next_token] += 1\n",
        "    total_next = sum(next_token_counts[given_token_id].values())\n",
        "    if total_next == 0:\n",
        "        return {}\n",
        "    probabilities = {\n",
        "        tokenizer.convert_ids_to_tokens(token_id): count / total_next\n",
        "        for token_id, count in next_token_counts[given_token_id].items()\n",
        "    }\n",
        "    return probabilities\n",
        "\n",
        "def compute_next_token_counts(tokens):\n",
        "    next_token_counts = defaultdict(Counter)\n",
        "    for current_token, next_token in zip(tokens[:-1], tokens[1:]):\n",
        "        next_token_counts[current_token][next_token] += 1\n",
        "    return next_token_counts\n",
        "\n",
        "def prepare_token_data(sentences, tokenizer=None):\n",
        "    if tokenizer is None:\n",
        "        print(\"No tokenizer provided. Creating a new tokenizer.\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    tokens = []\n",
        "    for sentence in tqdm(sentences, desc=\"Tokenizing...\"):\n",
        "        token_ids = tokenizer.encode(sentence, add_special_tokens=False)\n",
        "        tokens.extend(token_ids)\n",
        "    return tokens\n",
        "\n",
        "def sample_next_token(next_counts):\n",
        "    tokens, counts = zip(*next_counts.items())\n",
        "    total = sum(counts)\n",
        "    probabilities = [count / total for count in counts]\n",
        "    return random.choices(tokens, weights=probabilities, k=1)[0]\n",
        "\n",
        "def random_sample_generate_sentence(next_token_counts, start_token_text, tokenizer=None, max_length=20):\n",
        "    if tokenizer is None:\n",
        "        print(\"No tokenizer provided. Creating a new tokenizer.\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    current_token_id = tokenizer.convert_tokens_to_ids(start_token_text.lower())\n",
        "    generated_tokens = [current_token_id]\n",
        "    print(\"Generating a sentence with random sampling...\")\n",
        "    for _ in tqdm(range(max_length)):\n",
        "        next_counts = next_token_counts.get(current_token_id, None)\n",
        "        if not next_counts:\n",
        "            break  # No next token found\n",
        "        next_token_id = sample_next_token(next_counts)\n",
        "        generated_tokens.append(next_token_id)\n",
        "        current_token_id = next_token_id\n",
        "        token_text = tokenizer.convert_ids_to_tokens(current_token_id)\n",
        "        if token_text in ['.', '!', '?', tokenizer.sep_token, tokenizer.pad_token]:\n",
        "            break\n",
        "    generated_text = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(generated_tokens))\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17yQaqeOfxgP"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "import wikipediaapi\n",
        "\n",
        "wiki_wiki = wikipediaapi.Wikipedia('MyProjectName', 'en',\n",
        "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
        ")\n",
        "\n",
        "p_wiki = wiki_wiki.page(\"Breakfast\")\n",
        "text = p_wiki.text\n",
        "sentences = clean_text_data(text)\n",
        "\n",
        "start_token_text = 'breakfast'\n",
        "tokens = prepare_token_data(sentences, tokenizer=tokenizer)\n",
        "next_token_counts = compute_next_token_counts(tokens)\n",
        "generated_sentence = random_sample_generate_sentence(next_token_counts, start_token_text)\n",
        "print(\"Generated sentence (random sample):\")\n",
        "print(generated_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEJe_sTafxgQ"
      },
      "source": [
        "### 더 많은 데이터!\n",
        "\n",
        "간단한 해결책은, 더 많은 텍스트 데이터를 사용하는 것입니다! 데이터는 많을 수록, 실제 언어 모델과 비슷해집니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mwparserfromhell\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "rZTHZB6Og7EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "wikipedia_dataset = load_dataset('wikipedia', '20220301.en')\n",
        "print(wikipedia_dataset)"
      ],
      "metadata": {
        "id": "58wgBjhdhGH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "num_total_data = 1000\n",
        "wikipedia_text_combined = ''\n",
        "for i, data in enumerate(tqdm(wikipedia_dataset['train'])):\n",
        "    wikipedia_text_combined += data['text']\n",
        "    if i > num_total_data:\n",
        "        break"
      ],
      "metadata": {
        "id": "_ADOE3NShjbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "wikipedia_sentences = clean_text_data(wikipedia_text_combined)\n",
        "tokens = prepare_token_data(wikipedia_sentences, tokenizer=tokenizer)\n",
        "next_token_counts = compute_next_token_counts(tokens)"
      ],
      "metadata": {
        "id": "fIVTkHxK4xwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_token_text = 'The'\n",
        "generated_sentence = random_sample_generate_sentence(\n",
        "    next_token_counts, start_token_text,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "print(\"Generated sentence (random sample):\")\n",
        "print(generated_sentence)"
      ],
      "metadata": {
        "id": "fQI6g3hO71ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unmtduwwfxgQ"
      },
      "source": [
        "### 학습으로 넘어가기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# Prepare dataset\n",
        "class NextTokenDataset(Dataset):\n",
        "    def __init__(self, sentences, tokenizer):\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "        for sentence in sentences:\n",
        "            tokens = tokenizer.encode(sentence, add_special_tokens=True)\n",
        "            for i in range(len(tokens) - 1):\n",
        "                self.inputs.append(tokens[i])\n",
        "                self.targets.append(tokens[i+1])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.inputs[idx]), torch.tensor(self.targets[idx])\n",
        "\n",
        "# Define simple MLP model\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim=128):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        return self.mlp(x)"
      ],
      "metadata": {
        "id": "14k-KEahg7en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Dataset and DataLoader\n",
        "dataset = NextTokenDataset(wikipedia_sentences, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Model, Loss, Optimizer\n",
        "model = SimpleMLP(vocab_size=tokenizer.vocab_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for prev_token, next_token in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(prev_token)\n",
        "        loss = criterion(logits, next_token)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "XPXHim-nAQEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference example\n",
        "def generate_sequence(model, tokenizer, start_token, max_len=10):\n",
        "    model.eval()\n",
        "    token = tokenizer.encode(start_token, add_special_tokens=False)[-1]\n",
        "    output_tokens = [token]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        input_tensor = torch.tensor([token])\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_tensor)\n",
        "            next_token = torch.argmax(logits, dim=-1).item()\n",
        "            output_tokens.append(next_token)\n",
        "            token = next_token\n",
        "\n",
        "    return tokenizer.decode(output_tokens)\n",
        "\n",
        "# Example generation\n",
        "print(generate_sequence(model, tokenizer, \"The\"))"
      ],
      "metadata": {
        "id": "PUjc5oRIARrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9Xu2h2wfxgQ"
      },
      "source": [
        "### (선택) 문맥의 흐름을 파악하여 자연스러운 문장 만들기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset\n",
        "class NextTokenDataset(Dataset):\n",
        "    def __init__(self, sentences, tokenizer, max_seq_len=32):\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "        for sentence in sentences:\n",
        "            tokens = tokenizer.encode(sentence, add_special_tokens=True)\n",
        "            for i in range(1, len(tokens)):\n",
        "                input_seq = tokens[:i]\n",
        "                target = tokens[i]\n",
        "                if len(input_seq) > max_seq_len:\n",
        "                    input_seq = input_seq[-max_seq_len:]\n",
        "                self.inputs.append(input_seq)\n",
        "                self.targets.append(target)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_seq = self.inputs[idx]\n",
        "        target = self.targets[idx]\n",
        "        return torch.tensor(input_seq), torch.tensor(target)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs, targets = zip(*batch)\n",
        "    inputs = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    targets = torch.stack(targets)\n",
        "    return inputs, targets\n",
        "\n",
        "# Define LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim=128, num_layers=1):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, _ = self.lstm(x)\n",
        "        output = output[:, -1, :]  # take the output of the last token\n",
        "        return self.fc(output)"
      ],
      "metadata": {
        "id": "TJG3SOMIg71l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Dataset and DataLoader\n",
        "dataset = NextTokenDataset(sentences, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Model, Loss, Optimizer\n",
        "lstm = LSTMModel(vocab_size=tokenizer.vocab_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lstm.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for input_seq, next_token in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "        optimizer.zero_grad()\n",
        "        logits = lstm(input_seq)\n",
        "        loss = criterion(logits, next_token)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "OXIAjaASBM5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference example\n",
        "def generate_sequence(model, tokenizer, start_text, max_len=10):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.encode(start_text, add_special_tokens=False)\n",
        "    tokens = torch.tensor(tokens).unsqueeze(0)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            logits = model(tokens)\n",
        "            next_token = torch.argmax(logits, dim=-1).item()\n",
        "            tokens = torch.cat([tokens, torch.tensor([[next_token]])], dim=1)\n",
        "\n",
        "    return tokenizer.decode(tokens.squeeze().tolist())\n",
        "\n",
        "# Example generation\n",
        "print(generate_sequence(lstm, tokenizer, \"The\"))"
      ],
      "metadata": {
        "id": "Z0zhnkgcBMhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC12TGrAfxgQ"
      },
      "source": [
        "### 문제점 논의하기"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hdm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}