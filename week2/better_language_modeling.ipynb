{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 언어 모델의 기초적 이해 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지난 시간 복습\n",
    "\n",
    "지난 시간에 우리는 텍스트 데이터를 분석해서, 각 단어 다음에 등장할 확률을 직접 세어서 구한 다음, 계산한 확률을 토대로 문장을 생성해 나갔습니다. \n",
    "\n",
    "여기에서, 우리는 수많은 문제점을 체험할 수 있었습니다: \n",
    "\n",
    "1. 우리가 가지고 있는 텍스트 데이터에 시작 단어가 존재하지 않는 경우, 문장을 생성하지 못합니다.\n",
    "2. 문장을 생성하더라도, 문맥의 흐름을 파악하지 못하고 자연스럽지 않은 문장이 생성되기도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지난 시간의 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def split_text_to_sentences(text):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return sentences\n",
    "\n",
    "def remove_text_from_start_end_marker(text, start_marker='(', end_marker=')'):\n",
    "    return re.sub(r'\\{}.*?\\{}'.format(re.escape(start_marker), re.escape(end_marker)), '', text).strip()\n",
    "\n",
    "def clean_text_data(text):\n",
    "    sentences = split_text_to_sentences(text)\n",
    "    sentences = [i.lower() for i in sentences] # make sentence lower cased. e.g. \"Hello World\" -> \"hello world\"\n",
    "    sentences = [remove_text_from_start_end_marker(i) for i in sentences] # remove parentheses and their content. e.g. \"hello world (test)\" -> \"hello world\"\n",
    "    short_sentences = []\n",
    "    for i in sentences:\n",
    "        temp = i.split(',')\n",
    "        for j in temp:\n",
    "            short_sentences.append(j.strip())\n",
    "    to_replace = [\"!\", \";\", '\\n', '</p>', '<a', 'id=', \"href=\", 'title=', 'class=', '</a>', '(', ')', '}', '{',\n",
    "                  '</sup>', '<p>', '</b>', '<sup', '>', '<', '\\\\', '-']\n",
    "    replace_with = ''\n",
    "    cleaned_sentences = []\n",
    "    for i in short_sentences:\n",
    "        word_array = i.split()\n",
    "        word_array_new = []\n",
    "        for word in word_array:\n",
    "            for to_replace_val in to_replace:\n",
    "                word = word.replace(to_replace_val, replace_with)\n",
    "            word_array_new.append(word)\n",
    "        cleaned_sentence = ' '.join(word_array_new).strip()\n",
    "        cleaned_sentence = re.sub(r'\\s+', ' ', cleaned_sentence) # Remove extra whitespaces\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "    cleaned_sentences = [i for i in cleaned_sentences if len(i.split()) > 10]\n",
    "    return cleaned_sentences\n",
    "\n",
    "def compute_next_token_probabilities(sentences, given_token_text, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        token_ids = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        tokens.extend(token_ids)\n",
    "    given_token_id = tokenizer.convert_tokens_to_ids(given_token_text)\n",
    "    next_token_counts = defaultdict(Counter)\n",
    "    for current_token, next_token in zip(tokens[:-1], tokens[1:]):\n",
    "        next_token_counts[current_token][next_token] += 1\n",
    "    total_next = sum(next_token_counts[given_token_id].values())\n",
    "    if total_next == 0:\n",
    "        return {}\n",
    "    probabilities = {\n",
    "        tokenizer.convert_ids_to_tokens(token_id): count / total_next\n",
    "        for token_id, count in next_token_counts[given_token_id].items()\n",
    "    }\n",
    "    return probabilities\n",
    "\n",
    "def compute_next_token_counts(tokens):\n",
    "    next_token_counts = defaultdict(Counter)\n",
    "    for current_token, next_token in zip(tokens[:-1], tokens[1:]):\n",
    "        next_token_counts[current_token][next_token] += 1\n",
    "    return next_token_counts\n",
    "\n",
    "def prepare_token_data(sentences, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        token_ids = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        tokens.extend(token_ids)\n",
    "    return tokens\n",
    "\n",
    "def sample_next_token(next_counts):\n",
    "    tokens, counts = zip(*next_counts.items())\n",
    "    total = sum(counts)\n",
    "    probabilities = [count / total for count in counts]\n",
    "    return random.choices(tokens, weights=probabilities, k=1)[0]\n",
    "\n",
    "def random_sample_generate_sentence(sentences, start_token_text, tokenizer=None, max_length=20):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokens = prepare_token_data(sentences, tokenizer=tokenizer)\n",
    "    next_token_counts = compute_next_token_counts(tokens)\n",
    "    current_token_id = tokenizer.convert_tokens_to_ids(start_token_text)\n",
    "    generated_tokens = [current_token_id]\n",
    "    for _ in range(max_length):\n",
    "        next_counts = next_token_counts.get(current_token_id, None)\n",
    "        if not next_counts:\n",
    "            break  # No next token found\n",
    "        next_token_id = sample_next_token(next_counts)\n",
    "        generated_tokens.append(next_token_id)\n",
    "        current_token_id = next_token_id\n",
    "        token_text = tokenizer.convert_ids_to_tokens(current_token_id)\n",
    "        if token_text in ['.', '!', '?', tokenizer.sep_token, tokenizer.pad_token]:\n",
    "            break\n",
    "    generated_text = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(generated_tokens))\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "import wikipediaapi\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia('MyProjectName', 'en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "p_wiki = wiki_wiki.page(\"Breakfast\")\n",
    "text = p_wiki.text\n",
    "sentences = clean_text_data(text)\n",
    "\n",
    "start_token_text = 'breakfast'\n",
    "generated_sentence = random_sample_generate_sentence(sentences, start_token_text)\n",
    "print(\"Generated sentence (random sample):\")\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 더 많은 데이터!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습으로 넘어가기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문맥의 흐름을 파악하여 자연스러운 문장 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제점 논의하기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
